{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Scrape HTML Content From a Page\n",
    "\n",
    "- **Static Websites**\n",
    "- _Hidden Websites_\n",
    "- _Dynamic Websites_\n",
    "\n",
    "In this course, you will work with a static website. You will also get a high-level overview about the challenges of scraping dynamically generated information and data behind logins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚠️ Durabilty Warning ⚠️\n",
    "\n",
    "Like [mentioned in the course](https://realpython.com/lessons/challenge-of-durability/), websites frequently change. Unfortunately the job board that you'll see in the course, indeed.com, has started to block scraping of their site since the recording of the course.\n",
    "\n",
    "Just like in the associated written tutorial on [web scraping with beautiful soup](https://realpython.com/beautiful-soup-web-scraper-python/#scrape-the-fake-python-job-site), you can instead use [Real Python's fake jobs site](https://realpython.github.io/fake-jobs/) to practice scraping a static website.\n",
    "\n",
    "All the concepts discussed in the course lessons are still accurate. Translating what you see onto a different website will be a good learning opportunity where you'll have to synthesize the information and apply it practically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static Websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://realpython.github.io/fake-jobs/\"\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.content[400:500]  # let's take a peek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have access to the data from this website. Can you already search for the content that interests you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try a string search\n",
    "loc = str(response.content).find(\"python\")\n",
    "loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.content[loc - 50 : loc + 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what about regex?\n",
    "import re\n",
    "\n",
    "re.findall(r\"python\", str(response.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works, but it is tedious and inefficient! That's where **parsing** and `BeautifulSoup` comes in and makes your life easier. You will learn more about that in the next chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Websites\n",
    "\n",
    "Some pages require you to log in before they display information. Scraping them _without_ logging in doesn't give you what you want. `requests` includes ways to authenticate with websites.\n",
    "\n",
    "Train this with [GitHub](https://github.com/) and our tutorial on [Python's Requests Library](https://realpython.com/python-requests/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(\"https://api.github.com/user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.status_code  # whoops! not authorized!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Websites\n",
    "\n",
    "Websites attempt to offload computing power to the client. That means they send back **JavaScript** code that the client's browser executes. These pages are harder to scrape, because that code needs to be executed before you will see the information you are interested in.\n",
    "\n",
    "Train this with [requests-html](https://requests.readthedocs.io/projects/requests-html/en/latest/) or [Selenium](https://selenium-python.readthedocs.io/) and our tutorial for [Modern Web Automation with Python and Selenium](https://realpython.com/modern-web-automation-with-python-and-selenium/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(\"https://twitter.com/search?q=realpython\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.status_code  # the code says all is fine..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.content  # ... but the content doesn't contain what you're looking for!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
